{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Tabnet Package and Import Tabnet Model:","metadata":{}},{"cell_type":"code","source":"!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:43:05.523269Z","iopub.execute_input":"2024-12-17T05:43:05.523953Z","iopub.status.idle":"2024-12-17T05:43:47.274141Z","shell.execute_reply.started":"2024-12-17T05:43:05.523912Z","shell.execute_reply":"2024-12-17T05:43:47.273212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetRegressor\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:43:47.275724Z","iopub.execute_input":"2024-12-17T05:43:47.275977Z","iopub.status.idle":"2024-12-17T05:43:51.293843Z","shell.execute_reply.started":"2024-12-17T05:43:47.275953Z","shell.execute_reply":"2024-12-17T05:43:51.292941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Import Important Libraries:**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:43:51.294971Z","iopub.execute_input":"2024-12-17T05:43:51.295336Z","iopub.status.idle":"2024-12-17T05:44:06.281235Z","shell.execute_reply.started":"2024-12-17T05:43:51.295309Z","shell.execute_reply":"2024-12-17T05:44:06.280333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = ['None', 'Mild', 'Moderate', 'Severe']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:06.282953Z","iopub.execute_input":"2024-12-17T05:44:06.283574Z","iopub.status.idle":"2024-12-17T05:44:06.287865Z","shell.execute_reply.started":"2024-12-17T05:44:06.283545Z","shell.execute_reply":"2024-12-17T05:44:06.286853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing Data:","metadata":{}},{"cell_type":"markdown","source":"### Discrete Data Analysis:","metadata":{}},{"cell_type":"markdown","source":"Read dataset","metadata":{}},{"cell_type":"code","source":"train = pl.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pl.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n\ndisplay(train)\ndisplay(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:06.288994Z","iopub.execute_input":"2024-12-17T05:44:06.289339Z","iopub.status.idle":"2024-12-17T05:44:06.433052Z","shell.execute_reply.started":"2024-12-17T05:44:06.2893Z","shell.execute_reply":"2024-12-17T05:44:06.432274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As can be seen in file ***data_dictionary.csv***, all the Season-related columns are **string** type. Since we have 4 seasons only, we wanna convert it into **categorical** data (***enum***).","metadata":{}},{"cell_type":"code","source":"season_dtype = pl.Enum(['Spring', 'Summer', 'Fall', 'Winter'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:06.434145Z","iopub.execute_input":"2024-12-17T05:44:06.43449Z","iopub.status.idle":"2024-12-17T05:44:06.446078Z","shell.execute_reply.started":"2024-12-17T05:44:06.434445Z","shell.execute_reply":"2024-12-17T05:44:06.445429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train.with_columns(pl.col('^.*Season$').cast(season_dtype))\ntest = test.with_columns(pl.col('^.*Season$').cast(season_dtype))\n\ndisplay(train)\ndisplay(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:06.447029Z","iopub.execute_input":"2024-12-17T05:44:06.447285Z","iopub.status.idle":"2024-12-17T05:44:06.538047Z","shell.execute_reply.started":"2024-12-17T05:44:06.44726Z","shell.execute_reply":"2024-12-17T05:44:06.537284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that at most columns (especially the target label columns ***ssi***), there are many missing values. , Since this is a supervised-learning problem, we have to:\n\n* Accept removing data samples where label ***ssi*** is not available(*null*).\n* From our new dataset(validlabel_dataset) containing only rows where sii is available, we **count** the missing values of each other feature and plot them.","metadata":{}},{"cell_type":"code","source":"validlabel_dataset = (\n    train\n    .filter(pl.col('sii').is_not_null())\n)\n\nmissing_count = (\n    validlabel_dataset\n    .null_count()\n    .transpose(include_header=True,\n               header_name='feature',\n               column_names=['null_count'])\n    .sort('null_count', descending=True)\n    .with_columns((pl.col('null_count') / len(validlabel_dataset)).alias('null_ratio'))\n)\n\n# Visualize missing data\nplt.figure(figsize=(6, 15))\nplt.title(f'Missing values over the {len(validlabel_dataset)} samples which have a target')\nplt.barh(np.arange(len(missing_count)), missing_count.get_column('null_ratio'), color='red', label='missing')\nplt.barh(np.arange(len(missing_count)), \n         1 - missing_count.get_column('null_ratio'),\n         left=missing_count.get_column('null_ratio'),\n         color='blue', label='available')\nplt.yticks(np.arange(len(missing_count)), missing_count.get_column('feature'))\nplt.gca().xaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\nplt.xlim(0, 1)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:06.539158Z","iopub.execute_input":"2024-12-17T05:44:06.539503Z","iopub.status.idle":"2024-12-17T05:44:07.536105Z","shell.execute_reply.started":"2024-12-17T05:44:06.539467Z","shell.execute_reply":"2024-12-17T05:44:07.535193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As can be seen from the labels, half of the samples are in class 0, while very few in class 3.\n\n=> We can infer the dataset seems imbalanced.","metadata":{}},{"cell_type":"code","source":"# Convert to Pandas for plotting\nvalue_counts_pd = validlabel_dataset.to_pandas()\n\n# Plot the bar chart\nvalue_counts_pd['sii'].value_counts(normalize=True).plot(kind='bar')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:07.537421Z","iopub.execute_input":"2024-12-17T05:44:07.537808Z","iopub.status.idle":"2024-12-17T05:44:07.783963Z","shell.execute_reply.started":"2024-12-17T05:44:07.537768Z","shell.execute_reply":"2024-12-17T05:44:07.783138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are also missing values in test dataset","metadata":{}},{"cell_type":"code","source":"print('Columns missing in test:')\nprint([f for f in train.columns if f not in test.columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:07.787577Z","iopub.execute_input":"2024-12-17T05:44:07.788058Z","iopub.status.idle":"2024-12-17T05:44:07.792629Z","shell.execute_reply.started":"2024-12-17T05:44:07.788032Z","shell.execute_reply":"2024-12-17T05:44:07.791827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"Here, we plot and visualize some features. Remember that we have 2 main features dtype, which are categorical type(categorical int and enum) and numeric type (int and float).","metadata":{}},{"cell_type":"markdown","source":"**1. Categorical Features:**","metadata":{}},{"cell_type":"markdown","source":"- Raw Train Dataset:","metadata":{}},{"cell_type":"code","source":"vc = train.get_column('Basic_Demos-Enroll_Season').value_counts()\nplt.pie(vc.get_column('count'), labels=vc.get_column('Basic_Demos-Enroll_Season'))\nplt.title('Season of enrollment')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:07.79355Z","iopub.execute_input":"2024-12-17T05:44:07.793797Z","iopub.status.idle":"2024-12-17T05:44:07.92258Z","shell.execute_reply.started":"2024-12-17T05:44:07.793774Z","shell.execute_reply":"2024-12-17T05:44:07.921377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- validlabel_dataset:","metadata":{}},{"cell_type":"code","source":"vc = validlabel_dataset.get_column('Basic_Demos-Enroll_Season').value_counts()\nplt.pie(vc.get_column('count'), labels=vc.get_column('Basic_Demos-Enroll_Season'))\nplt.title('Season of enrollment')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:07.924016Z","iopub.execute_input":"2024-12-17T05:44:07.924465Z","iopub.status.idle":"2024-12-17T05:44:08.067553Z","shell.execute_reply.started":"2024-12-17T05:44:07.924412Z","shell.execute_reply":"2024-12-17T05:44:08.066407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Raw Train Dataset:","metadata":{}},{"cell_type":"code","source":"vc = train.get_column('Basic_Demos-Sex').value_counts()\nplt.pie(vc.get_column('count'), labels=['boys', 'girls'])\nplt.title('Sex of participant')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:08.068821Z","iopub.execute_input":"2024-12-17T05:44:08.06927Z","iopub.status.idle":"2024-12-17T05:44:08.204084Z","shell.execute_reply.started":"2024-12-17T05:44:08.069221Z","shell.execute_reply":"2024-12-17T05:44:08.202978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- validlabel_dataset:","metadata":{}},{"cell_type":"code","source":"vc = validlabel_dataset.get_column('Basic_Demos-Sex').value_counts()\nplt.pie(vc.get_column('count'), labels=['boys', 'girls'])\nplt.title('Sex of participant')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:08.20568Z","iopub.execute_input":"2024-12-17T05:44:08.206099Z","iopub.status.idle":"2024-12-17T05:44:08.339851Z","shell.execute_reply.started":"2024-12-17T05:44:08.20605Z","shell.execute_reply":"2024-12-17T05:44:08.338829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2.  Numeric Features:**","metadata":{}},{"cell_type":"markdown","source":"- Raw Train Dataset:","metadata":{}},{"cell_type":"code","source":"_, axs = plt.subplots(2, 1, sharex=True)\nfor sex in range(2):\n    ax = axs.ravel()[sex]\n    vc = train.filter(pl.col('Basic_Demos-Sex') == sex).get_column('Basic_Demos-Age').value_counts()\n    ax.bar(vc.get_column('Basic_Demos-Age'),\n           vc.get_column('count'),\n           color=['lime', 'coral'][sex],\n           label=['boys', 'girls'][sex])\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.set_ylabel('count')\n    ax.legend()\nplt.suptitle('Age distribution')\naxs.ravel()[1].set_xlabel('years')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:08.340971Z","iopub.execute_input":"2024-12-17T05:44:08.341389Z","iopub.status.idle":"2024-12-17T05:44:09.025788Z","shell.execute_reply.started":"2024-12-17T05:44:08.341337Z","shell.execute_reply":"2024-12-17T05:44:09.025018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- validlabel_dataset:","metadata":{}},{"cell_type":"code","source":"_, axs = plt.subplots(2, 1, sharex=True)\nfor sex in range(2):\n    ax = axs.ravel()[sex]\n    vc = validlabel_dataset.filter(pl.col('Basic_Demos-Sex') == sex).get_column('Basic_Demos-Age').value_counts()\n    ax.bar(vc.get_column('Basic_Demos-Age'),\n           vc.get_column('count'),\n           color=['lime', 'coral'][sex],\n           label=['boys', 'girls'][sex])\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.set_ylabel('count')\n    ax.legend()\nplt.suptitle('Age distribution')\naxs.ravel()[1].set_xlabel('years')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:09.027099Z","iopub.execute_input":"2024-12-17T05:44:09.027442Z","iopub.status.idle":"2024-12-17T05:44:09.413939Z","shell.execute_reply.started":"2024-12-17T05:44:09.027402Z","shell.execute_reply":"2024-12-17T05:44:09.413024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Bivariate Analysis:","metadata":{}},{"cell_type":"markdown","source":"- Correlation Matrix:","metadata":{}},{"cell_type":"markdown","source":"* Starting from fundamental belief that each Feature contributes to deciding the final ouput result(label 'sii'). Hence, they might have some Relationship with the label, we call this 'Correlation'. Moreover, this 'Correlation' might exist among features.\n* Here, we estimate the Correlation Matrix which calculates the relation among features, and keep the correlation between each single feature and target feature (PCIAT-PCIAT_Total), because this feature decides the ssi values. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, 12))\n# Correlation Matrix\ncorr_matrix = validlabel_dataset.select([\n    'PCIAT-PCIAT_Total', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'Physical-BMI', \n    'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n    'Physical-Diastolic_BP', 'Physical-Systolic_BP', 'Physical-HeartRate',\n    'PreInt_EduHx-computerinternet_hoursday', 'SDS-SDS_Total_T', 'PAQ_A-PAQ_A_Total',\n    'PAQ_C-PAQ_C_Total', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec',\n    'FGC-FGC_CU', 'FGC-FGC_GSND','FGC-FGC_GSD','FGC-FGC_PU','FGC-FGC_SRL','FGC-FGC_SRR','FGC-FGC_TL','BIA-BIA_Activity_Level_num', \n    'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n    'BIA-BIA_FFMI','BIA-BIA_FMI', 'BIA-BIA_Fat','BIA-BIA_Frame_num','BIA-BIA_ICW','BIA-BIA_LDM','BIA-BIA_LST',\n    'BIA-BIA_SMM','BIA-BIA_TBW'\n    # Add other relevant columns\n]).to_pandas().corr()\n\nsii_corr = corr_matrix['PCIAT-PCIAT_Total'].drop('PCIAT-PCIAT_Total') # Drop correlation between PCIAT-PCIAT_Total and PCIAT-PCIAT_Total\nfiltered_corr = sii_corr[(sii_corr > 0.1) | (sii_corr < -0.1)]\n\n#Plot the correlation matrix\nplt.figure(figsize=(8, 6))\nfiltered_corr.sort_values().plot(kind='barh', color='lightcoral')\nplt.title('Correlation > 0.1 or < -0.1 with PCIAT-PCIAT_Total')\nplt.xlabel('Correlation coefficient')\nplt.ylabel('Features')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:09.415042Z","iopub.execute_input":"2024-12-17T05:44:09.415322Z","iopub.status.idle":"2024-12-17T05:44:09.734995Z","shell.execute_reply.started":"2024-12-17T05:44:09.415295Z","shell.execute_reply":"2024-12-17T05:44:09.734102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Continuous Data Analysis ( Time Series):","metadata":{}},{"cell_type":"markdown","source":"We analyze and visualize actigraphy data for a specific individual identified by their id. Actigraphy data, typically collected from wearable devices, measures movement (via acceleration) and other related metrics (e.g., light exposure). The aim is to inspect and understand behavioral patterns over time by examining specific features, with options to focus on a single week or simplify the analysis.","metadata":{}},{"cell_type":"markdown","source":"1. Understand Behavioral Patterns:\n* By plotting metrics like acceleration (X, Y, Z), light exposure, and derived measures (norm, enmo), we gain insights into daily activity trends.\n* The function filters and highlights relevant data, such as periods when the wearable device was actively used.\n\n2. Inspect Subsets of Data:\n\nAllows filtering the data to focus on a smaller subset, such as one week's worth of activity.\nCan visualize specific features to simplify or tailor analysis for specific use cases.\n\n3. Support Data Cleaning:\n\nBy visualizing metrics like non-wear_flag, analysts can ensure that unwanted or invalid data points are excluded from further analysis.","metadata":{}},{"cell_type":"code","source":"def analyze_actigraphy(actigraphy='/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id={id}/part-0.parquet', id=0, only_one_week=False, small=False):\n\n    day = actigraphy.get_column('relative_date_PCIAT') + actigraphy.get_column('time_of_day') / 86400e9\n    sample = train.filter(pl.col('id') == id)\n    age = sample.get_column('Basic_Demos-Age').item()\n    sex = ['boy', 'girl'][sample.get_column('Basic_Demos-Sex').item()]\n    actigraphy = (\n        actigraphy\n        .with_columns(\n            (day.diff() * 86400).alias('diff_seconds'),\n            (np.sqrt(np.square(pl.col('X')) + np.square(pl.col('Y')) + np.square(pl.col('Z'))).alias('norm'))\n        )\n    )\n\n    if only_one_week:\n        start = np.ceil(day.min())\n        mask = (start <= day.to_numpy()) & (day.to_numpy() <= start + 7*3)\n        mask &= ~ actigraphy.get_column('non-wear_flag').cast(bool).to_numpy()\n    else:\n        mask = np.full(len(day), True)\n        \n    if small:\n        timelines = [\n            ('enmo', 'forestgreen'),\n            ('light', 'orange'),\n        ]\n    else:\n        timelines = [\n            ('X', 'c'),\n            ('Y', 'c'),\n            ('Z', 'c'),\n#             ('norm', 'c'),\n            ('enmo', 'lightgreen'),\n            ('anglez', 'lightblue'),\n            ('light', 'orange'),\n            ('non-wear_flag', 'brown')\n    #         ('diff_seconds', 'k'),\n        ]\n\n    # Plotting\n    _, axs = plt.subplots(len(timelines), 1, sharex=True, figsize=(12, len(timelines) * 1.1 + 0.5))\n    for ax, (feature, color) in zip(axs, timelines):\n        ax.set_facecolor('#eeeeee')\n        ax.scatter(day.to_numpy()[mask],\n                   actigraphy.get_column(feature).to_numpy()[mask],\n                   color=color, label=feature, s=1)\n        ax.legend(loc='upper left', facecolor='#eeeeee')\n        if feature == 'diff_seconds':\n            ax.set_ylim(-0.5, 20.5)\n    axs[-1].set_xlabel('day')\n    axs[-1].xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.tight_layout()\n    axs[0].set_title(f'id={id}, {sex}, age={age}')\n    plt.show()\n    \n\n# Import And Read Time Series Dataset\nactigraphy = pl.read_parquet('/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=0417c91e/part-0.parquet')\n# display(actigraphy)\nanalyze_actigraphy(actigraphy,'064e8da5', only_one_week=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:09.736014Z","iopub.execute_input":"2024-12-17T05:44:09.736285Z","iopub.status.idle":"2024-12-17T05:44:11.823178Z","shell.execute_reply.started":"2024-12-17T05:44:09.736259Z","shell.execute_reply":"2024-12-17T05:44:11.822276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"* Processes all files in a directory using multithreading (ThreadPoolExecutor and tqdm for progress tracking).\n* Returns a DataFrame with summary statistics for each file.\n","metadata":{}},{"cell_type":"code","source":"def process_file(filename, dirname):\n    dataset = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    dataset.drop('step', axis=1, inplace=True)\n    return dataset.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    dataset = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    dataset['id'] = indexes\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:11.824469Z","iopub.execute_input":"2024-12-17T05:44:11.824741Z","iopub.status.idle":"2024-12-17T05:44:11.830618Z","shell.execute_reply.started":"2024-12-17T05:44:11.824714Z","shell.execute_reply":"2024-12-17T05:44:11.829622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This below Encoder Class Learns representation of each feature, the relationship among them.\n* The Encoder learns relation of features (including the missing ones).\n* The Decoder retrieves the original data (return the full dataset, and it will label the null values). \n\nAt the end, we have a so-called 'pretrained' model (The Encoder)","metadata":{}},{"cell_type":"markdown","source":"> *Note: We must **NOT** call the Decoder. It is just for testing if dataset is restored as origin or not*","metadata":{}},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, input_dim*2),\n            nn.ReLU(),\n            nn.Linear(input_dim*2, input_dim*3),\n            nn.ReLU(),\n            nn.Linear(input_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ndef Encoding(dataset, encoding_dim=50, epochs=50, batch_size=32):\n    # Standardalize data\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(dataset)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n\n    # Setup Encoder\n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss() # Loss Function\n    optimizer = optim.Adam(autoencoder.parameters()) # Optimizer\n\n    # 'Training' to extract features representations\n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n\n    # After having the Encoder learnt parameters and feature representations, call Encoder to train the real model\n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:11.831853Z","iopub.execute_input":"2024-12-17T05:44:11.832127Z","iopub.status.idle":"2024-12-17T05:44:11.848685Z","shell.execute_reply.started":"2024-12-17T05:44:11.832103Z","shell.execute_reply":"2024-12-17T05:44:11.847926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Read our dataset:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntrain_id = train_ts.drop('id', axis=1)\ntest_id = test_ts.drop('id', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:44:11.849618Z","iopub.execute_input":"2024-12-17T05:44:11.84989Z","iopub.status.idle":"2024-12-17T05:45:21.254117Z","shell.execute_reply.started":"2024-12-17T05:44:11.849866Z","shell.execute_reply":"2024-12-17T05:45:21.253289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Train AutoEncoder for **time series** dataset:","metadata":{}},{"cell_type":"code","source":"train_ts_encoded = Encoding(train_id, encoding_dim=60, epochs=100, batch_size=32) # Output will be 60 columns(features)\ntest_ts_encoded = Encoding(test_id, encoding_dim=60, epochs=100, batch_size=32)# Output will be 60 columns(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:21.255149Z","iopub.execute_input":"2024-12-17T05:45:21.255444Z","iopub.status.idle":"2024-12-17T05:45:31.931051Z","shell.execute_reply.started":"2024-12-17T05:45:21.255416Z","shell.execute_reply":"2024-12-17T05:45:31.930174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_series_cols = train_ts_encoded.columns.tolist()\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\n# Merge time series encoded with train and test\ntrain = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n\n# imputer: Fill in the Missing values in numeric columns using a KNNImputer.\n# For each missing value,looks at the 5 nearest rows and computes the mean of the corresponding values to replace the missing value.\n# Remember if it cannot find missing values based on 5 nearest neighbors, its value is set to inf\nimputer = KNNImputer(n_neighbors=6)\nnumeric_cols = train.select_dtypes(include=['int32', 'int64', 'float64']).columns\nimputed_data = imputer.fit_transform(train[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\nfor col in train.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train[col]\n\n# Assign the numeric columns with no missing values into train (mean train loses categorical columns)\n# Note: We'll have to add categorical columns later\ntrain = train_imputed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:31.932298Z","iopub.execute_input":"2024-12-17T05:45:31.93334Z","iopub.status.idle":"2024-12-17T05:45:38.514224Z","shell.execute_reply.started":"2024-12-17T05:45:31.933298Z","shell.execute_reply":"2024-12-17T05:45:38.513537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Next step is Feature Engineering. We will create new columns from 2 old columns. These new columns will have features of both, plus its own information (hidden features). That way, our can learn and give better performance.","metadata":{}},{"cell_type":"code","source":"def feature_engineering(data):\n    season_cols = [col for col in data.columns if 'Season' in col]\n    data = data.drop(season_cols, axis=1) \n    data['BMI_Age'] = data['Physical-BMI'] * data['Basic_Demos-Age']\n    data['Internet_Hours_Age'] = data['PreInt_EduHx-computerinternet_hoursday'] * data['Basic_Demos-Age']\n    data['BMI_Internet_Hours'] = data['Physical-BMI'] * data['PreInt_EduHx-computerinternet_hoursday']\n    data['BFP_BMI'] = data['BIA-BIA_Fat'] / data['BIA-BIA_BMI']\n    data['FFMI_BFP'] = data['BIA-BIA_FFMI'] / data['BIA-BIA_Fat']\n    data['FMI_BFP'] = data['BIA-BIA_FMI'] / data['BIA-BIA_Fat']\n    data['LST_TBW'] = data['BIA-BIA_LST'] / data['BIA-BIA_TBW']\n    data['BFP_BMR'] = data['BIA-BIA_Fat'] * data['BIA-BIA_BMR']\n    data['BFP_DEE'] = data['BIA-BIA_Fat'] * data['BIA-BIA_DEE']\n    data['BMR_Weight'] = data['BIA-BIA_BMR'] / data['Physical-Weight']\n    data['DEE_Weight'] = data['BIA-BIA_DEE'] / data['Physical-Weight']\n    data['SMM_Height'] = data['BIA-BIA_SMM'] / data['Physical-Height']\n    data['Muscle_to_Fat'] = data['BIA-BIA_SMM'] / data['BIA-BIA_FMI']\n    data['Hydration_Status'] = data['BIA-BIA_TBW'] / data['Physical-Weight']\n    data['ICW_TBW'] = data['BIA-BIA_ICW'] / data['BIA-BIA_TBW']\n    data['BMI_PHR'] = data['Physical-BMI'] * data['Physical-HeartRate']\n    data['SDS_InternetHours'] = data['SDS-SDS_Total_T'] * data['PreInt_EduHx-computerinternet_hoursday']\n    \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.51542Z","iopub.execute_input":"2024-12-17T05:45:38.515725Z","iopub.status.idle":"2024-12-17T05:45:38.522415Z","shell.execute_reply.started":"2024-12-17T05:45:38.515696Z","shell.execute_reply":"2024-12-17T05:45:38.521543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = feature_engineering(train)\ntrain = train.dropna(thresh=10, axis=0)\ntest = feature_engineering(test)\n\ntrain = train.drop('id', axis=1)\ntest  = test .drop('id', axis=1)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.523352Z","iopub.execute_input":"2024-12-17T05:45:38.5236Z","iopub.status.idle":"2024-12-17T05:45:38.562189Z","shell.execute_reply.started":"2024-12-17T05:45:38.523576Z","shell.execute_reply":"2024-12-17T05:45:38.561315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Next, we add missing columns(categorical columns) except for 'sii'","metadata":{}},{"cell_type":"code","source":"featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-CGAS_Score', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'BMI_PHR', 'SDS_InternetHours']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\n\nfeaturesCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-CGAS_Score', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T',\n                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'BMI_PHR', 'SDS_InternetHours']\n\nfeaturesCols += time_series_cols\ntest = test[featuresCols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.563394Z","iopub.execute_input":"2024-12-17T05:45:38.564189Z","iopub.status.idle":"2024-12-17T05:45:38.574596Z","shell.execute_reply.started":"2024-12-17T05:45:38.564146Z","shell.execute_reply":"2024-12-17T05:45:38.573949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Since there is inf values in our dataset, we need to handle them:","metadata":{}},{"cell_type":"code","source":"if np.any(np.isinf(train)):\n    train = train.replace([np.inf, -np.inf], np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.575509Z","iopub.execute_input":"2024-12-17T05:45:38.575782Z","iopub.status.idle":"2024-12-17T05:45:38.588065Z","shell.execute_reply.started":"2024-12-17T05:45:38.575758Z","shell.execute_reply":"2024-12-17T05:45:38.587444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models Build","metadata":{}},{"cell_type":"markdown","source":"Here, we use below models. These models all have strong multi-classfication, which are widely used.\n* **LightXGBM**.\r* **XGBoost**.\n* **CatBoost**.\n* **Voting Regressor**.cy.","metadata":{}},{"cell_type":"markdown","source":"* Setup Parameters for **LightGBM**, **XGBoost** and **CatBoost**","metadata":{}},{"cell_type":"code","source":"SEED = 42\nn_splits = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.591427Z","iopub.execute_input":"2024-12-17T05:45:38.591714Z","iopub.status.idle":"2024-12-17T05:45:38.607555Z","shell.execute_reply.started":"2024-12-17T05:45:38.591688Z","shell.execute_reply":"2024-12-17T05:45:38.606766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01,  # Increased from 2.68e-06\n    'device': 'cpu'\n\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED,\n    'tree_method': 'gpu_hist',\n\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10,  # Increase this value\n    'task_type': 'GPU'\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.60853Z","iopub.execute_input":"2024-12-17T05:45:38.608849Z","iopub.status.idle":"2024-12-17T05:45:38.619792Z","shell.execute_reply.started":"2024-12-17T05:45:38.608824Z","shell.execute_reply":"2024-12-17T05:45:38.619162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also call the class **TabNetWrapper** that applies **TabNet** Architecture. It will train our model using Tabnet, and then save the best performance state.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_tabnet.callbacks import Callback\nimport os\nimport torch\nfrom pytorch_tabnet.callbacks import Callback\n\nclass TabNetWrapper(BaseEstimator, RegressorMixin):\n    def __init__(self, **kwargs):\n        self.model = TabNetRegressor(**kwargs)\n        self.kwargs = kwargs\n        self.imputer = SimpleImputer(strategy='median') # Handle missing values, replace them with median\n        self.best_model_path = 'best_model.pt'\n        \n    def fit(self, X, y):       \n        # Replace null values with cols median.\n        X_imputed = self.imputer.fit_transform(X)\n        \n        if hasattr(y, 'values'):\n            y = y.values\n            \n        # Separate inputs and labels of train and test\n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_imputed, \n            y, \n            test_size=0.2,\n            random_state=42\n        )\n        \n        # Train TabNet model\n        history = self.model.fit(\n            X_train=X_train,\n            y_train=y_train.reshape(-1, 1), # Ensure y_train and y_valid has the correct shape for TabNet\n            eval_set=[(X_valid, y_valid.reshape(-1, 1))], # Monitor best model\n            eval_name=['valid'],\n            eval_metric=['mse'],# Use Mean Squared Error as the evaluation metric\n            max_epochs=500, # Number of epoch\n            patience=50,# Stop early if no validation improvement for 50 epochs\n            batch_size=1024, # Batch size for train\n            virtual_batch_size=128, # Batch size for gradient updates\n            num_workers=0,\n            drop_last=False,\n            # Save the best model into file .pth\n            callbacks=[\n                TabNetPretrainedModelCheckpoint(\n                    filepath=self.best_model_path,\n                    monitor='valid_mse',\n                    mode='min',\n                    save_best_only=True,\n                    verbose=True\n                )\n            ]\n        )\n        \n        # Load the best model\n        if os.path.exists(self.best_model_path):\n            self.model.load_model(self.best_model_path)\n            os.remove(self.best_model_path)  # Remove temporary file\n\n        self.feature_importances_ = self.model.feature_importances_\n        \n        return self\n    \n    def predict(self, X):\n        X_imputed = self.imputer.transform(X)\n        return self.model.predict(X_imputed).flatten()\n    \n    def __deepcopy__(self, memo):\n        # Add deepcopy support for scikit-learn\n        cls = self.__class__\n        result = cls.__new__(cls)\n        memo[id(self)] = result\n        for k, v in self.__dict__.items():\n            setattr(result, k, deepcopy(v, memo))\n        return result\n\n# TabNet hyperparameters\nTabNet_Params = {\n    'n_d': 64,              # Width of the decision prediction layer\n    'n_a': 64,              # Width of the attention embedding for each step\n    'n_steps': 5,           # Number of steps in the architecture\n    'gamma': 1.5,           # Coefficient for feature selection regularization\n    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n    'lambda_sparse': 1e-4,  # Sparsity regularization\n    'optimizer_fn': torch.optim.Adam,\n    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n    'mask_type': 'entmax',\n    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n    'verbose': 1,\n    'device_name': 'cuda' if torch.cuda.is_available() else cpu\n}\n\nclass TabNetPretrainedModelCheckpoint(Callback):\n    def __init__(self, filepath, monitor='val_loss', mode='min', \n                 save_best_only=True, verbose=1):\n        super().__init__()  # Initialize parent class\n        self.filepath = filepath\n        self.monitor = monitor\n        self.mode = mode\n        self.save_best_only = save_best_only\n        self.verbose = verbose\n        self.best = float('inf') if mode == 'min' else -float('inf')\n        \n    def on_train_begin(self, logs=None):\n        self.model = self.trainer  # Use trainer itself as model\n        \n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        current = logs.get(self.monitor)\n        if current is None:\n            return\n        \n        # Check if current metric is better than best\n        if (self.mode == 'min' and current < self.best) or \\\n           (self.mode == 'max' and current > self.best):\n            if self.verbose:\n                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n            self.best = current\n            if self.save_best_only:\n                self.model.save_model(self.filepath)  # Save the entire model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.621076Z","iopub.execute_input":"2024-12-17T05:45:38.621515Z","iopub.status.idle":"2024-12-17T05:45:38.638812Z","shell.execute_reply.started":"2024-12-17T05:45:38.621477Z","shell.execute_reply":"2024-12-17T05:45:38.638094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train And Evaluation:","metadata":{}},{"cell_type":"markdown","source":"> **All of our training process is implemented on GPU.**","metadata":{}},{"cell_type":"markdown","source":"* Evaluation Metrics:","metadata":{}},{"cell_type":"code","source":"# if np.any(np.isinf(train)):\n#     train = train.replace([np.inf, -np.inf], np.nan)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.639844Z","iopub.execute_input":"2024-12-17T05:45:38.640729Z","iopub.status.idle":"2024-12-17T05:45:38.652865Z","shell.execute_reply.started":"2024-12-17T05:45:38.640689Z","shell.execute_reply":"2024-12-17T05:45:38.652153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Train Step:","metadata":{}},{"cell_type":"markdown","source":"* Cross-Validation: Stratified K-Folds cross-validation is employed to split the data into training and validation sets, ensuring balanced class distribution in each fold.\n  \r* \nQuadratic Weighted Kappa (QWK): The performance of the models is evaluated using QWK, which measures the agreement between predicted and actual values, taking into account the ordinal nature of the target variable\n  \n* \r\nThreshold Optimization: The minimize function from scipy.optimize is used to fine-tune decision thresholds that map continuous predictions to discrete categories (None, Mild, Moderate, Severe).","metadata":{}},{"cell_type":"code","source":"# feature_col = train.drop(['sii'], axis=1).columns\n# # 各モデルの平均重要度を結合（Noneでなければ）\n# all_importances = pd.DataFrame({'feature': feature_col})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.653761Z","iopub.execute_input":"2024-12-17T05:45:38.654072Z","iopub.status.idle":"2024-12-17T05:45:38.664899Z","shell.execute_reply.started":"2024-12-17T05:45:38.654035Z","shell.execute_reply":"2024-12-17T05:45:38.664087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def TrainML(model_class, test_data):\n#     X = train.drop(['sii'], axis=1)\n#     y = train['sii']\n\n#     SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n#     train_S = []\n#     test_S = []\n    \n#     oof_non_rounded = np.zeros(len(y), dtype=float) \n#     oof_rounded = np.zeros(len(y), dtype=int) \n#     test_preds = np.zeros((len(test_data), n_splits))\n\n#         # 特徴量名取得\n#     feature_names = X.columns\n\n#     # ===== ここでリストを初期化する =====\n#     lgb_importances_list = []\n#     xgb_importances_list = []\n#     cat_importances_list = []\n#     tabnet_importances_list = []\n\n#     for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n#         X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n#         y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n#         model = clone(model_class)\n#         model.fit(X_train, y_train)\n\n#         y_train_pred = model.predict(X_train)\n#         y_val_pred = model.predict(X_val)\n\n#         oof_non_rounded[test_idx] = y_val_pred\n#         y_val_pred_rounded = y_val_pred.round(0).astype(int)\n#         oof_rounded[test_idx] = y_val_pred_rounded\n\n#         train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n#         val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n#         train_S.append(train_kappa)\n#         test_S.append(val_kappa)\n        \n#         test_preds[:, fold] = model.predict(test_data)\n        \n#         print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n#         clear_output(wait=True)\n\n#         # 各Fold終了後、ベースモデルから特徴量重要度を取得\n#         # model.estimators_は[('lightgbm', Light), ('xgboost', XGB_Model), ('catboost', CatBoost_Model), ('tabnet', TabNet_Model)]などと対応\n#         named_estimators = model.named_estimators_\n        \n#         # LightGBM\n#         if 'lightgbm' in named_estimators and hasattr(named_estimators['lightgbm'], 'feature_importances_'):\n#             lgb_importances_list.append(named_estimators['lightgbm'].feature_importances_)\n\n#         # XGBoost\n#         if 'xgboost' in named_estimators and hasattr(named_estimators['xgboost'], 'feature_importances_'):\n#             xgb_importances_list.append(named_estimators['xgboost'].feature_importances_)\n\n#         # CatBoost\n#         if 'catboost' in named_estimators and hasattr(named_estimators['catboost'], 'get_feature_importance'):\n#             cat_importances_list.append(named_estimators['catboost'].get_feature_importance())\n\n#         # TabNet\n#         if 'tabnet' in named_estimators and hasattr(named_estimators['tabnet'], 'feature_importances_'):\n#             tabnet_importances_list.append(named_estimators['tabnet'].feature_importances_)\n\n\n#     print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n#     print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n#     KappaOPtimizer = minimize(evaluate_predictions,\n#                               x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n#                               method='Nelder-Mead')\n#     assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n#     oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n#     tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n#     print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n#     tpm = test_preds.mean(axis=1)\n#     tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n#     submission = pd.DataFrame({\n#         'id': sample['id'],\n#         'sii': tpTuned\n#     })\n\n#     # ====== 特徴量重要度の可視化 ======\n#     # 各Foldで取得した重要度を平均\n#     def mean_importances(importances_list):\n#         if len(importances_list) > 0:\n#             return np.mean(importances_list, axis=0)\n#         else:\n#             return None\n\n#     lgb_mean = mean_importances(lgb_importances_list)\n#     xgb_mean = mean_importances(xgb_importances_list)\n#     cat_mean = mean_importances(cat_importances_list)\n#     tab_mean = mean_importances(tabnet_importances_list)\n\n#     # 各モデルの重要度を正規化\n#     def normalize_importances(importance_array):\n#         if importance_array is not None:\n#             return importance_array / importance_array.sum()\n#         else:\n#             return None\n\n#     lgb_mean_normalized = normalize_importances(lgb_mean)\n#     xgb_mean_normalized = normalize_importances(xgb_mean)\n#     cat_mean_normalized = normalize_importances(cat_mean)\n#     tab_mean_normalized = normalize_importances(tab_mean)\n\n\n#     # モデルごとに重要度を可視化\n#     # LightGBM重要度\n#     if lgb_mean_normalized is not None:\n#         lgb_df = pd.DataFrame({'feature': feature_names, 'importance': lgb_mean_normalized}).sort_values('importance', ascending=False)\n#         plt.figure(figsize=(10,20))\n#         plt.barh(lgb_df['feature'], lgb_df['importance'])\n#         plt.gca().invert_yaxis()\n#         plt.title(\"LightGBM Feature Importance\")\n#         plt.show()\n#         all_importances['LightGBM'] = lgb_mean_normalized\n\n#     # XGBoost重要度\n#     if xgb_mean_normalized is not None:\n#         xgb_df = pd.DataFrame({'feature': feature_names, 'importance': xgb_mean_normalized}).sort_values('importance', ascending=False)\n#         plt.figure(figsize=(10,20))\n#         plt.barh(xgb_df['feature'], xgb_df['importance'])\n#         plt.gca().invert_yaxis()\n#         plt.title(\"XGBoost Feature Importance\")\n#         plt.show()\n#         all_importances['XGBoost'] = xgb_mean_normalized\n\n#     # CatBoost重要度\n#     if cat_mean_normalized is not None:\n#         cat_df = pd.DataFrame({'feature': feature_names, 'importance': cat_mean_normalized}).sort_values('importance', ascending=False)\n#         plt.figure(figsize=(10,20))\n#         plt.barh(cat_df['feature'], cat_df['importance'])\n#         plt.gca().invert_yaxis()\n#         plt.title(\"CatBoost Feature Importance\")\n#         plt.show()\n#         all_importances['CatBoost'] = cat_mean_normalized\n\n#     # TabNet重要度\n#     if tab_mean_normalized is not None:\n#         tab_df = pd.DataFrame({'feature': feature_names, 'importance': tab_mean_normalized}).sort_values('importance', ascending=False)\n#         plt.figure(figsize=(10,20))\n#         plt.barh(tab_df['feature'], tab_df['importance'])\n#         plt.gca().invert_yaxis()\n#         plt.title(\"TabNet Feature Importance\")\n#         plt.show()\n#         all_importances['TabNet'] = tab_mean_normalized\n\n#     return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.665981Z","iopub.execute_input":"2024-12-17T05:45:38.666228Z","iopub.status.idle":"2024-12-17T05:45:38.678983Z","shell.execute_reply.started":"2024-12-17T05:45:38.66619Z","shell.execute_reply":"2024-12-17T05:45:38.678277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:45:38.679892Z","iopub.execute_input":"2024-12-17T05:45:38.68012Z","iopub.status.idle":"2024-12-17T05:45:38.69431Z","shell.execute_reply.started":"2024-12-17T05:45:38.680098Z","shell.execute_reply":"2024-12-17T05:45:38.693602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Setup for the Models:","metadata":{}},{"cell_type":"code","source":"# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\nTabNet_Model = TabNetWrapper(**TabNet_Params) # New","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:48:38.726415Z","iopub.execute_input":"2024-12-17T05:48:38.726802Z","iopub.status.idle":"2024-12-17T05:48:38.740018Z","shell.execute_reply.started":"2024-12-17T05:48:38.72677Z","shell.execute_reply":"2024-12-17T05:48:38.739312Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **Here, We use Ensemble Learning: Voting Regressor. This approach is beneficial as it leverages the strengths of multiple models, reducing overfitting and improving overall model performance.**","metadata":{}},{"cell_type":"code","source":"voting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model),\n    ('tabnet', TabNet_Model)\n], weights=[4.0,4.0,5.0,4.0])\n\nSubmission1 = TrainML(voting_model, test)\n\nSubmission1.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:48:42.102837Z","iopub.execute_input":"2024-12-17T05:48:42.103292Z","iopub.status.idle":"2024-12-17T05:51:10.452624Z","shell.execute_reply.started":"2024-12-17T05:48:42.103237Z","shell.execute_reply":"2024-12-17T05:51:10.451778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Submission1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:51:10.454232Z","iopub.execute_input":"2024-12-17T05:51:10.454489Z","iopub.status.idle":"2024-12-17T05:51:10.466047Z","shell.execute_reply.started":"2024-12-17T05:51:10.454463Z","shell.execute_reply":"2024-12-17T05:51:10.465208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Second Models:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n        \ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)   \n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01  # Increased from 2.68e-06\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\n# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n# TabNet_Model = TabNetModel(**TabNet_Params)  # New:TAbNet\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n    # ('tabnet', TabNet_Model)  # New:TabNet\n])\n\n# Train the ensemble model\nSubmission2 = TrainML(voting_model, test)\n\n# Save submission\n#Submission2.to_csv('submission.csv', index=False)\nSubmission2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:51:10.46749Z","iopub.execute_input":"2024-12-17T05:51:10.46778Z","iopub.status.idle":"2024-12-17T05:53:05.574177Z","shell.execute_reply.started":"2024-12-17T05:51:10.46775Z","shell.execute_reply":"2024-12-17T05:53:05.573238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Third Model:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n\n    return tp_rounded\n\nimputer = SimpleImputer(strategy='median')\n\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n    # ('tabnet', Pipeline(steps=[('imputer', imputer), ('regressor', TabnetWrapper(**TabNet_Params))]))  # New:TabNet\n])\n\nSubmission3 = TrainML(ensemble, test)\n\nSubmission3 = TrainML(ensemble, test)\nSubmission3 = pd.DataFrame({\n    'id': sample['id'],\n    'sii': Submission3\n})\n\nSubmission3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:53:05.576037Z","iopub.execute_input":"2024-12-17T05:53:05.576319Z","iopub.status.idle":"2024-12-17T05:58:04.144851Z","shell.execute_reply.started":"2024-12-17T05:53:05.576293Z","shell.execute_reply":"2024-12-17T05:58:04.14402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Choose The Best Model:","metadata":{}},{"cell_type":"code","source":"sub1 = Submission1\nsub2 = Submission2\nsub3 = Submission3\n\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\n\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii']\n})\n\ndef majority_vote(row):\n    return row.mode()[0]\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Majority voting completed and saved to 'Final_Submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:58:04.145764Z","iopub.execute_input":"2024-12-17T05:58:04.146014Z","iopub.status.idle":"2024-12-17T05:58:04.160445Z","shell.execute_reply.started":"2024-12-17T05:58:04.145989Z","shell.execute_reply":"2024-12-17T05:58:04.159775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:58:04.16128Z","iopub.execute_input":"2024-12-17T05:58:04.161485Z","iopub.status.idle":"2024-12-17T05:58:04.174031Z","shell.execute_reply.started":"2024-12-17T05:58:04.161464Z","shell.execute_reply":"2024-12-17T05:58:04.173216Z"}},"outputs":[],"execution_count":null}]}